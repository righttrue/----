{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import ray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 개별 파일 병합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_concat_csv(file_pattern: str) -> pd.DataFrame:\n",
    "    file_list = glob.glob(file_pattern)\n",
    "    df_list = []\n",
    "\n",
    "    for file in file_list:\n",
    "        try:\n",
    "            df = pd.read_csv(file, low_memory=False)\n",
    "            df_list.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error of reading {file}: {e}\")\n",
    "\n",
    "    return pd.concat(df_list, ignore_index=True) if df_list else pd.DataFrame()\n",
    "\n",
    "\n",
    "def vaisala_road(s_status):\n",
    "    vaisala_road_state = {\n",
    "        '1': \"Dry\",\n",
    "        '2': \"Moist\",\n",
    "        '3': \"Wet\",\n",
    "        '5': \"Frost\",\n",
    "        '6': \"Snow\",\n",
    "        '7': \"Ice\",\n",
    "        '9': \"Slush\", \n",
    "    }\n",
    "\n",
    "    # Get the last character of the status\n",
    "    last_char = str(s_status)[-1]\n",
    "\n",
    "    # Check if the last character is in the mapping dictionary\n",
    "    if last_char not in vaisala_road_state:\n",
    "        return \"error\"\n",
    "    return vaisala_road_state[last_char]\n",
    "\n",
    "# read file from directory\n",
    "luft_total_df = load_and_concat_csv(\"../DATA/MWIS_2023_NDJF_OBS/*MARWIS*csv\")\n",
    "vi_total_df = load_and_concat_csv(\"../DATA/MWIS_2023_NDJF_OBS/*move4*csv\")\n",
    "\n",
    "# Apply the function to the DataFrame column\n",
    "vi_total_df[\"s_status_txt\"] = vi_total_df[\"s_status\"].apply(vaisala_road)\n",
    "\n",
    "# drop column no need\n",
    "drop_column_list = ['gdirection1','gdirection2','digitalcomp_x','digitalcomp_y','digitalcomp_z',\n",
    "                    'ta2','rh2','loggerta', 'batteryvolt', 'rev1', 'rev2']\n",
    "vi_total_df.drop(drop_column_list, axis = 1, inplace=True)\n",
    "\n",
    "# Extract components of the date and time\n",
    "year = vi_total_df['gdate'].astype(str).str[-6:-4].apply(lambda x: '20' + x if len(x) == 2 else x)  # Handling two-digit year\n",
    "month = vi_total_df['gdate'].astype(str).str[-4:-2]\n",
    "day = vi_total_df['gdate'].astype(str).str[-2:]\n",
    "\n",
    "hour = vi_total_df['gtime'].astype(str).str[-6:-4].str.zfill(2)\n",
    "minute = vi_total_df['gtime'].astype(str).str[-4:-2]\n",
    "second = vi_total_df['gtime'].astype(str).str[-2:]\n",
    "\n",
    "# Combine extracted components into a datetime string and convert to datetime\n",
    "vi_total_df['TIMESTAMP'] = pd.to_datetime(\n",
    "    year + '-' + month + '-' + day + ' ' + hour + ':' + minute + ':' + second,\n",
    "    format='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "vi_total_df['TIMESTAMP'] = pd.to_datetime(vi_total_df['TIMESTAMP'])\n",
    "luft_total_df['TIMESTAMP'] = pd.to_datetime(luft_total_df['TIMESTAMP'])\n",
    "\n",
    "total_df = pd.merge(vi_total_df, luft_total_df, on='TIMESTAMP', how='outer', suffixes=('_vi', '_luft'))\n",
    "\n",
    "total_df.replace({\n",
    "    '/': '',\n",
    "    '//////': np.nan,\n",
    "    '////': np.nan,\n",
    "    '///////': np.nan\n",
    "}, inplace=True)\n",
    "\n",
    "total_df.to_csv(\"../DATA/CONCAT/luft_vi_2023_2024.csv\", index=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 센서 자료 AWS자료 병합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import KDTree\n",
    "\n",
    "total_df['glatitude']=total_df['glatitude'].ffill() \n",
    "total_df['glongitude']=total_df['glongitude'].ffill() \n",
    "\n",
    "# AWS 정보 데이터프레임 읽기 및 k-d 트리 생성\n",
    "aws_info_df = pd.read_csv(\"../DATA/AWS/META_관측지점정보.csv\", encoding='cp949')\n",
    "aws_coord = aws_info_df.drop_duplicates(subset=[\"지점\"])[['지점', '위도', '경도']]\n",
    "aws_coord.columns = ['site', 'latitude', 'longitude']\n",
    "\n",
    "# 위도와 경도 점들을 배열로 생성\n",
    "points = np.array(list(zip(aws_coord['latitude'], aws_coord['longitude'])))\n",
    "tree = KDTree(points)\n",
    "\n",
    "# 질의 점들 생성\n",
    "query_points = total_df[['glatitude', 'glongitude']].to_numpy()\n",
    "\n",
    "# 가장 가까운 점들 찾기\n",
    "distances, indices = tree.query(query_points)\n",
    "\n",
    "# 가장 가까운 점의 사이트 정보 가져오기\n",
    "nearest_sites = aws_coord.iloc[indices]['site'].values\n",
    "\n",
    "# total_df에 AWS 사이트 정보 추가\n",
    "total_df['aws_site'] = nearest_sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-30 10:39:10,552\tINFO worker.py:1779 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "asw_stn_list = total_df['aws_site'].unique()\n",
    "\n",
    "aws_file_dir = \"C:/Users/user/Desktop/RoadAnalysis/DATA/AWS/202311~202403/\"\n",
    "aws_file_list = glob.glob(os.path.join(aws_file_dir,\"**/AWS*\"), recursive=True)\n",
    "# Ray 초기화\n",
    "ray.init()\n",
    "\n",
    "@ray.remote\n",
    "def process_file(file,asw_stn_list):\n",
    "    df = pd.read_csv(file, sep=\"#\", header=None,encoding='cp949',engine='python')\n",
    "    df.columns = ['STN_ID', 'TM', 'LAT', 'LON', 'HT', 'WD', 'WS', 'TA', 'HM', 'PA', 'PS', 'RN_YN',\n",
    "                  'RN_1HR', 'RN_DAY', 'RN_15M', 'RN_60M', 'WD_INS', 'WS_INS', \"END\"]\n",
    "    df = df[df['STN_ID'].isin(asw_stn_list)]\n",
    "    return df\n",
    "\n",
    "# 병렬로 파일 처리\n",
    "futures = [process_file.remote(file,asw_stn_list) for file in aws_file_list]\n",
    "dfs_list = ray.get(futures)\n",
    "\n",
    "# 데이터프레임 병합\n",
    "dfs = pd.concat(dfs_list)\n",
    "\n",
    "# Ray 종료\n",
    "ray.shutdown()\n",
    "\n",
    "# 결과 출력 또는 저장\n",
    "# print(dfs)\n",
    "dfs.to_csv(\"../DATA/AWS/aws_202311_202403.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS 자료와 센서 자료 결합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_df = pd.read_csv(\"../DATA/AWS/aws_202311_202403.csv\")\n",
    "# Rename the column 'STN_ID' to 'aws_site' in aws_df\n",
    "aws_df.rename(columns={\"STN_ID\": \"aws_site\"}, inplace=True)\n",
    "\n",
    "# Convert 'aws_site' to integer type in aws_df\n",
    "aws_df[\"aws_site\"] = aws_df[\"aws_site\"].astype(int)\n",
    "\n",
    "# Convert 'TM' to datetime and create 'min_time' column in aws_df\n",
    "aws_df['min_time'] = pd.to_datetime(aws_df['TM'], format='%Y%m%d%H%M')\n",
    "aws_df['min_time'] = aws_df['min_time'].dt.strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "# Convert 'TIMESTAMP' to datetime and create 'min_time' column in total_df\n",
    "total_df['min_time'] = pd.to_datetime(total_df['TIMESTAMP'], format='%Y%m%d%H%M')\n",
    "total_df['min_time'] = total_df['min_time'].dt.strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "# Convert 'aws_site' to integer type in total_df\n",
    "total_df[\"aws_site\"] = total_df[\"aws_site\"].astype(int)\n",
    "\n",
    "df_last = pd.merge(left=total_df,right=aws_df, on = [\"aws_site\",'min_time'], how = 'left')\n",
    "df_last.to_csv(\"../DATA/CONCAT/aws_sensor_merge.csv\",index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
